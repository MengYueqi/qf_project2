{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b6533ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_DIR  -> f:\\Master\\Book\\DSA5205\\Project 2\\code\\qf_project2\\data\n",
      "OUTPUT_DIR-> f:\\Master\\Book\\DSA5205\\Project 2\\code\\qf_project2\\GRU\\output\n",
      "PRICE_FILE exists: True\n"
     ]
    }
   ],
   "source": [
    "# ---------- Path config: data is sibling of GRU folder ----------\n",
    "from pathlib import Path\n",
    "\n",
    "# \"here\" = the folder where this notebook/script lives (GRU/)\n",
    "try:\n",
    "    HERE = Path(__file__).resolve().parent\n",
    "except NameError:\n",
    "    # In Jupyter notebooks __file__ is not defined; use current working dir\n",
    "    HERE = Path.cwd()\n",
    "\n",
    "PROJECT_ROOT = HERE.parent         # one level up (where GRU and data are siblings)\n",
    "DATA_DIR     = PROJECT_ROOT / \"data\"       # ../data\n",
    "OUTPUT_DIR   = HERE / \"output\"             # keep outputs under GRU/output\n",
    "PRICE_FILE   = DATA_DIR / \"price_data_full.csv\"\n",
    "\n",
    "# (optional) sanity check prints\n",
    "print(\"DATA_DIR  ->\", DATA_DIR)\n",
    "print(\"OUTPUT_DIR->\", OUTPUT_DIR)\n",
    "print(\"PRICE_FILE exists:\", PRICE_FILE.exists())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "243e7dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Tickers detected: 12; start from #1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f59dc63c60374b2eb3b693ad94595dbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "All tickers [1→12]:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[AAPL] saved -> AAPL.csv  (test period)\n",
      "[AAPL] TEST  MSE=2.985e+03 | R2=-2.296 | IC= 0.910 | Hit= 0.494 | Sharpe=-0.29\n",
      "[AAPL] saved -> AAPL_TEST_curve.png\n",
      "\n",
      "[AMZN] saved -> AMZN.csv  (test period)\n",
      "[AMZN] TEST  MSE=8.707e+02 | R2= 0.515 | IC= 0.949 | Hit= 0.494 | Sharpe=-0.35\n",
      "[AMZN] saved -> AMZN_TEST_curve.png\n",
      "\n",
      "[AVGO] saved -> AVGO.csv  (test period)\n",
      "[AVGO] TEST  MSE=1.343e+03 | R2= 0.788 | IC= 0.966 | Hit= 0.494 | Sharpe= 0.69\n",
      "[AVGO] saved -> AVGO_TEST_curve.png\n",
      "\n",
      "[GOOGL] saved -> GOOGL.csv  (test period)\n",
      "[GOOGL] TEST  MSE=1.630e+02 | R2= 0.875 | IC= 0.938 | Hit= 0.496 | Sharpe= 0.12\n",
      "[GOOGL] saved -> GOOGL_TEST_curve.png\n",
      "\n",
      "[META] saved -> META.csv  (test period)\n",
      "[META] TEST  MSE=4.153e+03 | R2= 0.872 | IC= 0.968 | Hit= 0.511 | Sharpe= 0.73\n",
      "[META] saved -> META_TEST_curve.png\n",
      "\n",
      "[MSFT] saved -> MSFT.csv  (test period)\n",
      "[MSFT] TEST  MSE=4.199e+02 | R2= 0.922 | IC= 0.967 | Hit= 0.496 | Sharpe= 0.02\n",
      "[MSFT] saved -> MSFT_TEST_curve.png\n",
      "\n",
      "[NFLX] saved -> NFLX.csv  (test period)\n",
      "[NFLX] TEST  MSE=8.976e+03 | R2= 0.904 | IC= 0.980 | Hit= 0.517 | Sharpe=-0.46\n",
      "[NFLX] saved -> NFLX_TEST_curve.png\n",
      "\n",
      "[NVDA] saved -> NVDA.csv  (test period)\n",
      "[NVDA] TEST  MSE=4.727e+03 | R2=-0.857 | IC= 0.974 | Hit= 0.514 | Sharpe= 1.04\n",
      "[NVDA] saved -> NVDA_TEST_curve.png\n",
      "\n",
      "[ORCL] saved -> ORCL.csv  (test period)\n",
      "[ORCL] TEST  MSE=4.011e+02 | R2= 0.855 | IC= 0.941 | Hit= 0.453 | Sharpe=-0.45\n",
      "[ORCL] saved -> ORCL_TEST_curve.png\n",
      "\n",
      "[QQQ] saved -> QQQ.csv  (test period)\n",
      "[QQQ] TEST  MSE=5.176e+04 | R2=-5.833 | IC= 0.970 | Hit= 0.528 | Sharpe= 0.52\n",
      "[QQQ] saved -> QQQ_TEST_curve.png\n",
      "\n",
      "[SPY] saved -> SPY.csv  (test period)\n",
      "[SPY] TEST  MSE=1.327e+04 | R2=-0.863 | IC= 0.953 | Hit= 0.520 | Sharpe=-0.25\n",
      "[SPY] saved -> SPY_TEST_curve.png\n",
      "\n",
      "[TSLA] saved -> TSLA.csv  (test period)\n",
      "[TSLA] TEST  MSE=1.131e+04 | R2=-0.858 | IC= 0.889 | Hit= 0.494 | Sharpe=-0.52\n",
      "[TSLA] saved -> TSLA_TEST_curve.png\n",
      "\n",
      "Saved OOS_summary.csv\n",
      "Ticker          MSE   R2_TEST       IC  HitRate    Sharpe TEST_start   TEST_end  #TEST_days\n",
      "  MSFT   419.872329  0.922072 0.967412 0.495739  0.023163 2023-01-03 2025-10-23         705\n",
      "  NFLX  8975.841259  0.904083 0.980365 0.517045 -0.463092 2023-01-03 2025-10-23         705\n",
      " GOOGL   163.019232  0.875107 0.938017 0.495739  0.124575 2023-01-03 2025-10-23         705\n",
      "  META  4152.888772  0.871824 0.968469 0.511364  0.732823 2023-01-03 2025-10-23         705\n",
      "  ORCL   401.069623  0.855414 0.941407 0.453125 -0.449151 2023-01-03 2025-10-23         705\n",
      "  AVGO  1343.058278  0.787588 0.966091 0.494318  0.690010 2023-01-03 2025-10-23         705\n",
      "  AMZN   870.741358  0.515037 0.949311 0.494318 -0.347156 2023-01-03 2025-10-23         705\n",
      "  NVDA  4726.824458 -0.856995 0.974481 0.514205  1.043777 2023-01-03 2025-10-23         705\n",
      "  TSLA 11308.545047 -0.858481 0.888627 0.494318 -0.518770 2023-01-03 2025-10-23         705\n",
      "   SPY 13265.178127 -0.863429 0.952589 0.519886 -0.249776 2023-01-03 2025-10-23         705\n",
      "  AAPL  2984.866840 -2.295622 0.909915 0.494318 -0.294980 2023-01-03 2025-10-23         705\n",
      "   QQQ 51763.893053 -5.832964 0.970215 0.528409  0.522148 2023-01-03 2025-10-23         705\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================\n",
    "# Multi-Ticker | GRU (classification) → Online calibration → r̂\n",
    "# Fixed split: Train/Valid/Test; train only on Train+Valid; rolling forecast on Test\n",
    "# - Iterate tickers in CSV (multi-index columns: [PriceField, Ticker])\n",
    "# - Features unified to the provided list (per-ticker, shifted by 1 to avoid leakage)\n",
    "# - Keep online ridge calibration + online mean matching\n",
    "# - \"Scheme A\" (rebasing-by-blocks) is for display/stats only\n",
    "# - For each ticker:\n",
    "#     * Train (≤ TRAIN_END) + Valid (TRAIN_END~VAL_END) → freeze weights\n",
    "#     * Rolling forecast on Test (> VAL_END); write {TICKER}.csv -> [Date, PredictedPrice]\n",
    "#     * Plot 1 chart \"Actual vs Predicted (rebased)\" and print OOS metrics (MSE, R2, IC, Hit, Sharpe)\n",
    "# - Extra output: OOS_summary.csv (test metrics summary for all tickers)\n",
    "# ==============================================================\n",
    "\n",
    "import os, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch, torch.nn as nn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm.auto import tqdm\n",
    "from collections import deque\n",
    "\n",
    "tqdm.monitor_interval = 0\n",
    "\n",
    "# ---------------- Config ----------------\n",
    "START_FROM_IDX = 0\n",
    "\n",
    "PRICE_FILE = os.path.join(DATA_DIR, \"price_data_full.csv\")\n",
    "\n",
    "START      = \"2010-01-01\"\n",
    "END        = None  # to last complete day\n",
    "\n",
    "# Fixed split boundaries\n",
    "TRAIN_END = \"2020-12-31\"\n",
    "VAL_END   = \"2022-12-31\"   # Testing is after VAL_END (exclusive)\n",
    "\n",
    "# Sequence & minimum history\n",
    "SEQ_LEN        = 20\n",
    "MIN_TRAIN_DAYS = 40\n",
    "\n",
    "# Training\n",
    "BATCH_SIZE   = 512\n",
    "LR           = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "MAX_EPOCHS   = 120\n",
    "PATIENCE     = 12\n",
    "HIDDEN_SIZE  = 64\n",
    "NUM_LAYERS   = 2\n",
    "DROPOUT      = 0.2\n",
    "CLIP_NORM    = 1.0\n",
    "\n",
    "# Calibration & safety caps\n",
    "RIDGE_L2      = 5e-6\n",
    "B_CAP         = 0.17\n",
    "RHAT_CAP      = 0.023\n",
    "CAL_WIN       = 40\n",
    "MIN_CAL_SAMPLES = 40\n",
    "\n",
    "# Online mean matching\n",
    "MEAN_MATCH_WIN = 40\n",
    "MEAN_MATCH_MIN = 20\n",
    "\n",
    "# Display-only: Scheme A (rebasing by blocks)\n",
    "REBASE_DAYS_FOR_CURVE = 20\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "if device.type == \"cuda\":\n",
    "    try: torch.set_float32_matmul_precision(\"high\")\n",
    "    except: pass\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "def last_complete_day():\n",
    "    return (pd.Timestamp.today().normalize() - pd.Timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "if END is None: END = last_complete_day()\n",
    "\n",
    "# ---------------- Simple AdamW (pure PyTorch, decoupled weight decay) ----------------\n",
    "class SimpleAdamW:\n",
    "    def __init__(self, params, lr=1e-3, weight_decay=1e-4, betas=(0.9, 0.999), eps=1e-8):\n",
    "        plist = list(params)\n",
    "        self.params = [p for p in plist if isinstance(p, torch.Tensor) and p.requires_grad]\n",
    "        if not self.params:\n",
    "            raise ValueError(\"Model has no trainable parameters.\")\n",
    "        self.lr, self.wd, self.betas, self.eps = lr, weight_decay, betas, eps\n",
    "        self.state = {}\n",
    "\n",
    "    def zero_grad(self, set_to_none=True):\n",
    "        for p in self.params:\n",
    "            if p.grad is not None:\n",
    "                if set_to_none: p.grad = None\n",
    "                else: p.grad.detach_(); p.grad.zero_()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        b1, b2 = self.betas\n",
    "        for p in self.params:\n",
    "            g = p.grad\n",
    "            if g is None: continue\n",
    "            if self.wd:\n",
    "                p.data.mul_(1 - self.lr * self.wd)\n",
    "            st = self.state.get(p)\n",
    "            if st is None:\n",
    "                st = self.state[p] = {\n",
    "                    \"t\": 0,\n",
    "                    \"m\": torch.zeros_like(p, memory_format=torch.preserve_format),\n",
    "                    \"v\": torch.zeros_like(p, memory_format=torch.preserve_format),\n",
    "                }\n",
    "            st[\"t\"] += 1; t = st[\"t\"]\n",
    "            st[\"m\"].mul_(b1).add_(g, alpha=1 - b1)\n",
    "            st[\"v\"].mul_(b2).addcmul_(g, g, value=1 - b2)\n",
    "            m_hat = st[\"m\"] / (1 - b1**t)\n",
    "            v_hat = st[\"v\"] / (1 - b2**t)\n",
    "            p.data.addcdiv_(m_hat, v_hat.sqrt().add_(self.eps), value=-self.lr)\n",
    "\n",
    "# ---------------- Indicators & helpers ----------------\n",
    "def rsi(series, n=14):\n",
    "    d = series.diff()\n",
    "    up = d.clip(lower=0); down = -d.clip(upper=0)\n",
    "    ma_up = up.rolling(n, min_periods=n).mean()\n",
    "    ma_dn = down.rolling(n, min_periods=n).mean()\n",
    "    rs = ma_up / ma_dn\n",
    "    out = 100 - (100/(1+rs))\n",
    "    return out/100.0  # scaled to 0~1\n",
    "\n",
    "def macd(close, fast=12, slow=26, signal=9):\n",
    "    ema_f = close.ewm(span=fast, adjust=False).mean()\n",
    "    ema_s = close.ewm(span=slow, adjust=False).mean()\n",
    "    line  = ema_f - ema_s\n",
    "    sig   = line.ewm(span=signal, adjust=False).mean()\n",
    "    hist  = line - sig\n",
    "    return line, sig, hist\n",
    "\n",
    "def average_true_range(high, low, close, n=14):\n",
    "    prev_close = close.shift(1)\n",
    "    tr = pd.concat([\n",
    "        (high - low).abs(),\n",
    "        (high - prev_close).abs(),\n",
    "        (low  - prev_close).abs()\n",
    "    ], axis=1).max(axis=1)\n",
    "    atr = tr.rolling(n, min_periods=n).mean()\n",
    "    return atr\n",
    "\n",
    "def online_mean_match(curr_rhat, ret_hist: deque, rhat_hist: deque):\n",
    "    if MEAN_MATCH_WIN is None or len(ret_hist) < MEAN_MATCH_MIN or len(rhat_hist) == 0:\n",
    "        return float(curr_rhat)\n",
    "    mu_true = float(np.mean(ret_hist))\n",
    "    mu_pred = float(np.mean(rhat_hist))\n",
    "    return float(curr_rhat + (mu_true - mu_pred))\n",
    "\n",
    "# ---------------- GRU ----------------\n",
    "class GRUTrend(nn.Module):\n",
    "    def __init__(self, in_dim, hidden=64, layers=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(input_size=in_dim, hidden_size=hidden, num_layers=layers,\n",
    "                          dropout=(dropout if layers>1 else 0.0), batch_first=True)\n",
    "        self.head = nn.Sequential(nn.Dropout(dropout), nn.Linear(hidden, 1))\n",
    "    def forward(self, x):\n",
    "        y, _ = self.gru(x)        # (B,T,H)\n",
    "        last = y[:, -1, :]\n",
    "        logit = self.head(last)   # (B,1)\n",
    "        return logit.squeeze(1)\n",
    "\n",
    "def make_seq_2d_to_3d(X2d: np.ndarray, seq_len: int):\n",
    "    Xs = []\n",
    "    for i in range(seq_len-1, len(X2d)):\n",
    "        Xs.append(X2d[i-seq_len+1:i+1, :])\n",
    "    return np.stack(Xs) if len(Xs)>0 else np.zeros((0, seq_len, X2d.shape[1]), dtype=X2d.dtype)\n",
    "\n",
    "def align_y(y: np.ndarray, seq_len: int):\n",
    "    return y[seq_len-1:]\n",
    "\n",
    "def get_by_key(series: pd.Series, key):\n",
    "    return series.iloc[key] if isinstance(key, (int, np.integer)) else series.loc[key]\n",
    "\n",
    "# ---------------- Online ridge calibrator (robust, sliding window) ----------------\n",
    "class OnlineCalibrator:\n",
    "    \"\"\"\n",
    "    Keeps a sliding window of (z, r), returns closed-form ridge solution for (a_t, b_t).\n",
    "    z = p - 0.5 ; r is next-day realized return (revealed with 1-day lag).\n",
    "    \"\"\"\n",
    "    def __init__(self, ridge=1e-6, cap_b=0.10, win=252):\n",
    "        self.ridge = ridge\n",
    "        self.cap_b = cap_b\n",
    "        self.win   = win if (win is None or win > 0) else None\n",
    "\n",
    "        # Do not use maxlen to avoid implicit pops that desync running sums.\n",
    "        self.buf = deque()\n",
    "\n",
    "        # Running sums\n",
    "        self.n = 0\n",
    "        self.sz = 0.0; self.sr = 0.0; self.szz = 0.0; self.szr = 0.0\n",
    "\n",
    "    def _push(self, z, r):\n",
    "        self.buf.append((z, r))\n",
    "        self.n  += 1\n",
    "        self.sz += z\n",
    "        self.sr += r\n",
    "        self.szz += z*z\n",
    "        self.szr += z*r\n",
    "\n",
    "    def _pop_left(self):\n",
    "        if self.win is None:\n",
    "            return\n",
    "        while self.n > self.win:\n",
    "            oldz, oldr = self.buf.popleft()\n",
    "            self.n  -= 1\n",
    "            self.sz -= oldz\n",
    "            self.sr -= oldr\n",
    "            self.szz -= oldz*oldz\n",
    "            self.szr -= oldz*oldr\n",
    "\n",
    "    def add(self, z, r):\n",
    "        self._push(float(z), float(r))\n",
    "        self._pop_left()\n",
    "\n",
    "    def fit_from_arrays(self, z_arr, r_arr):\n",
    "        if len(z_arr) != len(r_arr):\n",
    "            raise ValueError(\"z_arr and r_arr must have the same length\")\n",
    "        if self.win is not None and len(z_arr) > self.win:\n",
    "            z_arr = z_arr[-self.win:]\n",
    "            r_arr = r_arr[-self.win:]\n",
    "\n",
    "        # Rebuild buffer and recompute sums\n",
    "        self.buf.clear()\n",
    "        self.buf.extend((float(z), float(r)) for z, r in zip(z_arr, r_arr))\n",
    "        self.n = len(self.buf)\n",
    "\n",
    "        if self.n == 0:\n",
    "            self.sz = self.sr = self.szz = self.szr = 0.0\n",
    "            return\n",
    "\n",
    "        zs = [z for z, _ in self.buf]\n",
    "        rs = [r for _, r in self.buf]\n",
    "        self.sz  = float(np.sum(zs))\n",
    "        self.sr  = float(np.sum(rs))\n",
    "        self.szz = float(np.dot(zs, zs))\n",
    "        self.szr = float(np.dot(zs, rs))\n",
    "\n",
    "    def coef(self):\n",
    "        if self.n < MIN_CAL_SAMPLES:\n",
    "            return None\n",
    "        n, sz, sr, szz, szr = self.n, self.sz, self.sr, self.szz, self.szr\n",
    "        a00 = n + self.ridge\n",
    "        a01 = sz\n",
    "        a11 = szz + self.ridge\n",
    "        b0, b1 = sr, szr\n",
    "        det = a00*a11 - a01*a01\n",
    "        if det <= 1e-18:\n",
    "            return 0.0, 0.0\n",
    "        a = ( b0*a11 - b1*a01) / det\n",
    "        b = ( a00*b1 - a01*b0) / det\n",
    "        b = float(np.clip(b, -self.cap_b, self.cap_b))\n",
    "        return float(a), b\n",
    "\n",
    "# ---------------- Training with validation (early stopping) ----------------\n",
    "def train_with_val(X_train3, y_train1, X_val3, y_val1, n_feat_for_model):\n",
    "    ds_tr = torch.utils.data.TensorDataset(\n",
    "        torch.from_numpy(X_train3.astype(np.float32)),\n",
    "        torch.from_numpy(y_train1.astype(np.float32)))\n",
    "    ds_va = torch.utils.data.TensorDataset(\n",
    "        torch.from_numpy(X_val3.astype(np.float32)),\n",
    "        torch.from_numpy(y_val1.astype(np.float32)))\n",
    "    pin = (device.type == \"cuda\")\n",
    "    dl_tr = torch.utils.data.DataLoader(ds_tr, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                                        drop_last=False, pin_memory=pin)\n",
    "    dl_va = torch.utils.data.DataLoader(ds_va, batch_size=2048, shuffle=False,\n",
    "                                        drop_last=False, pin_memory=pin)\n",
    "\n",
    "    model = GRUTrend(n_feat_for_model, hidden=HIDDEN_SIZE, layers=NUM_LAYERS, dropout=DROPOUT).to(device)\n",
    "    pos_w = max(1.0, (len(y_train1)-y_train1.sum())/max(1.0, y_train1.sum()))\n",
    "    loss_fn = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([pos_w], device=device))\n",
    "    opt = SimpleAdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    best_state, best_val, bad = None, float(\"inf\"), 0\n",
    "    for _ in range(MAX_EPOCHS):\n",
    "        model.train()\n",
    "        for xb, yb in dl_tr:\n",
    "            xb, yb = xb.to(device, non_blocking=True), yb.to(device, non_blocking=True)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            loss = loss_fn(model(xb), yb)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), CLIP_NORM)\n",
    "            opt.step()\n",
    "\n",
    "        model.eval(); vs=[]\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in dl_va:\n",
    "                xb, yb = xb.to(device, non_blocking=True), yb.to(device, non_blocking=True)\n",
    "                vs.append(loss_fn(model(xb), yb).item())\n",
    "        v = float(np.mean(vs))\n",
    "        if v < best_val - 1e-9:\n",
    "            best_val, bad = v, 0\n",
    "            best_state = {k: w.detach().clone() for k, w in model.state_dict().items()}\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= PATIENCE: break\n",
    "\n",
    "    if best_state is not None: model.load_state_dict(best_state)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_proba(model, X3):\n",
    "    if len(X3) == 0: return np.array([], dtype=np.float32)\n",
    "    tens = torch.from_numpy(X3.astype(np.float32)).to(device)\n",
    "    return 1/(1+np.exp(-model(tens).cpu().numpy()))\n",
    "\n",
    "# ---------------- Feature builder (unified list) ----------------\n",
    "def build_unified_features(ticker, close_s, open_s, high_s, low_s, vol_s):\n",
    "    \"\"\"\n",
    "    Returns a DataFrame with columns named exactly as:\n",
    "      f'{ticker}_ret_5d', f'{ticker}_ret_20d', f'{ticker}_ret_60d', f'{ticker}_mom_accel',\n",
    "      f'{ticker}_rsi', f'{ticker}_macd_hist', f'{ticker}_ma20_bias', f'{ticker}_bb_position', f'{ticker}_ma_alignment',\n",
    "      f'{ticker}_vol_20d', f'{ticker}_vol_60d', f'{ticker}_atr',\n",
    "      f'{ticker}_volume_ratio', f'{ticker}_volume_mom',\n",
    "      f'{ticker}_52w_position', f'{ticker}_from_52w_high',\n",
    "      f'{ticker}_hl_spread'\n",
    "    All features are shifted by 1 day to avoid look-ahead bias.\n",
    "    \"\"\"\n",
    "    eps = 1e-12\n",
    "\n",
    "    # Returns for momentum & volatility\n",
    "    ret_1 = close_s.pct_change(1)\n",
    "    ret_5  = close_s.pct_change(5)\n",
    "    ret_20 = close_s.pct_change(20)\n",
    "    ret_60 = close_s.pct_change(60)\n",
    "\n",
    "    # Momentum group\n",
    "    mom_accel = ret_5 - ret_20  # short-term momentum minus medium-term\n",
    "\n",
    "    # Technical indicators\n",
    "    rsi14 = rsi(close_s, 14)  # scaled 0~1\n",
    "    _, _, macd_hist = macd(close_s)  # 12-26-9\n",
    "    ma20 = close_s.rolling(20).mean()\n",
    "    ma20_bias = close_s / (ma20 + eps) - 1.0\n",
    "\n",
    "    std20 = close_s.rolling(20).std()\n",
    "    upper = ma20 + 2*std20\n",
    "    lower = ma20 - 2*std20\n",
    "    bb_denom = (upper - lower).replace(0, np.nan)\n",
    "    bb_pos = (close_s - lower) / bb_denom\n",
    "    bb_pos = bb_pos.clip(0.0, 1.0).fillna(0.5)\n",
    "\n",
    "    ma5  = close_s.rolling(5).mean()\n",
    "    ma10 = close_s.rolling(10).mean()\n",
    "    ma_align = 0.5*(ma5/(ma10+eps) - 1.0) + 0.5*(ma10/(ma20+eps) - 1.0)\n",
    "\n",
    "    # Volatility\n",
    "    vol_20d = ret_1.rolling(20).std()\n",
    "    vol_60d = ret_1.rolling(60).std()\n",
    "    atr14 = average_true_range(high_s, low_s, close_s, 14) / (close_s + eps)\n",
    "\n",
    "    # Volume\n",
    "    v_ma20 = vol_s.rolling(20).mean()\n",
    "    volume_ratio = vol_s / (v_ma20 + eps)\n",
    "    volume_mom   = (vol_s.rolling(5).mean() / (v_ma20 + eps)) - 1.0\n",
    "\n",
    "    # Price position\n",
    "    hi_52w = close_s.rolling(252).max()\n",
    "    lo_52w = close_s.rolling(252).min()\n",
    "    range_52w = (hi_52w - lo_52w).replace(0, np.nan)\n",
    "    pos_52w = (close_s - lo_52w) / range_52w\n",
    "    pos_52w = pos_52w.clip(0.0, 1.0).fillna(0.5)\n",
    "    from_52w_high = close_s / (hi_52w + eps) - 1.0\n",
    "\n",
    "    # Microstructure\n",
    "    hl_spread = np.log(high_s/low_s)\n",
    "\n",
    "    # Assemble\n",
    "    cols = {\n",
    "        f\"{ticker}_ret_5d\":        ret_5,\n",
    "        f\"{ticker}_ret_20d\":       ret_20,\n",
    "        f\"{ticker}_ret_60d\":       ret_60,\n",
    "        f\"{ticker}_mom_accel\":     mom_accel,\n",
    "\n",
    "        f\"{ticker}_rsi\":           rsi14,\n",
    "        f\"{ticker}_macd_hist\":     macd_hist,\n",
    "        f\"{ticker}_ma20_bias\":     ma20_bias,\n",
    "        f\"{ticker}_bb_position\":   bb_pos,\n",
    "        f\"{ticker}_ma_alignment\":  ma_align,\n",
    "\n",
    "        f\"{ticker}_vol_20d\":       vol_20d,\n",
    "        f\"{ticker}_vol_60d\":       vol_60d,\n",
    "        f\"{ticker}_atr\":           atr14,\n",
    "\n",
    "        f\"{ticker}_volume_ratio\":  volume_ratio,\n",
    "        f\"{ticker}_volume_mom\":    volume_mom,\n",
    "\n",
    "        f\"{ticker}_52w_position\":  pos_52w,\n",
    "        f\"{ticker}_from_52w_high\": from_52w_high,\n",
    "\n",
    "        f\"{ticker}_hl_spread\":     hl_spread,\n",
    "    }\n",
    "    feat = pd.DataFrame(cols)\n",
    "    feat = feat.replace([np.inf, -np.inf], np.nan)\n",
    "    feat = feat.shift(1)  # critical: avoid look-ahead\n",
    "    return feat\n",
    "\n",
    "# ---------------- Load all data ----------------\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "if not os.path.exists(PRICE_FILE):\n",
    "    raise FileNotFoundError(f\"File not found: {PRICE_FILE}\")\n",
    "\n",
    "df = pd.read_csv(PRICE_FILE, header=[0,1], index_col=0)\n",
    "if isinstance(df.index[0], str) and str(df.index[0]).strip().lower() == \"date\":\n",
    "    df = df.iloc[1:]\n",
    "df.index = pd.to_datetime(df.index, errors=\"coerce\")\n",
    "df = df[~df.index.isna()].sort_index()\n",
    "df = df.loc[(df.index >= START) & (df.index <= END)]\n",
    "\n",
    "all_tickers = df.columns.get_level_values(1).unique().tolist()\n",
    "print(f\"Tickers detected: {len(all_tickers)}; start from #{START_FROM_IDX+1}\")\n",
    "\n",
    "summary = []\n",
    "\n",
    "# ---------------- Main loop per ticker ----------------\n",
    "for TICKER in tqdm(all_tickers[START_FROM_IDX:], desc=f\"All tickers [{START_FROM_IDX+1}→{len(all_tickers)}]\"):\n",
    "    try:\n",
    "        close_s = pd.to_numeric(df[(\"Close\",  TICKER)], errors=\"coerce\")\n",
    "        open_s  = pd.to_numeric(df[(\"Open\",   TICKER)], errors=\"coerce\")\n",
    "        high_s  = pd.to_numeric(df[(\"High\",   TICKER)], errors=\"coerce\")\n",
    "        low_s   = pd.to_numeric(df[(\"Low\",    TICKER)], errors=\"coerce\")\n",
    "        vol_s   = pd.to_numeric(df[(\"Volume\", TICKER)], errors=\"coerce\")\n",
    "    except KeyError:\n",
    "        print(f\"\\n[{TICKER}] missing fields, skip.\"); \n",
    "        continue\n",
    "\n",
    "    # Minimum length check\n",
    "    if close_s.dropna().shape[0] < (SEQ_LEN + MIN_TRAIN_DAYS + 50):\n",
    "        print(f\"\\n[{TICKER}] too few points, skip.\"); \n",
    "        continue\n",
    "\n",
    "    # Targets\n",
    "    y_ret = close_s.shift(-1)/close_s - 1.0\n",
    "    y_cls = (y_ret > 0).astype(int)\n",
    "\n",
    "    # Unified features (all shifted by 1)\n",
    "    feat = build_unified_features(TICKER, close_s, open_s, high_s, low_s, vol_s)\n",
    "\n",
    "    # Align and drop NaN\n",
    "    data = pd.concat([feat, y_ret.rename(\"y_ret\"), y_cls.rename(\"y_cls\"), close_s.rename(\"close\")], axis=1).dropna()\n",
    "    if data.shape[0] < (SEQ_LEN + MIN_TRAIN_DAYS + 10):\n",
    "        print(f\"\\n[{TICKER}] not enough aligned samples, skip.\"); \n",
    "        continue\n",
    "\n",
    "    X_all   = data.drop(columns=['y_ret','y_cls','close'])\n",
    "    ret_all = data['y_ret']\n",
    "    cls_all = data['y_cls'].astype(int)\n",
    "    close_all = data['close']\n",
    "    dates = X_all.index\n",
    "    n_feat = X_all.shape[1]\n",
    "\n",
    "    # Date-based split\n",
    "    train_idx = dates[dates <= pd.Timestamp(TRAIN_END)]\n",
    "    val_idx   = dates[(dates > pd.Timestamp(TRAIN_END)) & (dates <= pd.Timestamp(VAL_END))]\n",
    "    test_idx  = dates[dates > pd.Timestamp(VAL_END)]\n",
    "\n",
    "    if len(train_idx) < SEQ_LEN + MIN_TRAIN_DAYS or len(val_idx) < max(5, SEQ_LEN//2) or len(test_idx) < SEQ_LEN:\n",
    "        print(f\"\\n[{TICKER}] split too short (train/val/test), skip.\"); \n",
    "        continue\n",
    "\n",
    "    X_train = X_all.loc[train_idx]; y_train_cls = cls_all.loc[train_idx]; y_train_ret = ret_all.loc[train_idx]\n",
    "    X_val   = X_all.loc[val_idx];   y_val_cls   = cls_all.loc[val_idx];   y_val_ret   = ret_all.loc[val_idx]\n",
    "    X_test  = X_all.loc[test_idx];  y_test_ret  = ret_all.loc[test_idx]   # used for OOS metrics\n",
    "\n",
    "    # Scaler fit only on training\n",
    "    scaler = StandardScaler().fit(X_train.values)\n",
    "    Xtr2 = scaler.transform(X_train.values); Xva2 = scaler.transform(X_val.values)\n",
    "\n",
    "    # Sequences\n",
    "    Xtr3 = make_seq_2d_to_3d(Xtr2, SEQ_LEN); Xva3 = make_seq_2d_to_3d(Xva2, SEQ_LEN)\n",
    "    ytr1 = align_y(y_train_cls.values.astype(np.float32), SEQ_LEN)\n",
    "    yva1 = align_y(y_val_cls.values.astype(np.float32),   SEQ_LEN)\n",
    "\n",
    "    # Train + early stop on validation\n",
    "    model = train_with_val(Xtr3, ytr1, Xva3, yva1, n_feat_for_model=n_feat)\n",
    "\n",
    "    # Seed online calibrator using train+valid z,r (model frozen)\n",
    "    z_seed, r_seed = [], []\n",
    "    p_tr = predict_proba(model, make_seq_2d_to_3d(Xtr2, SEQ_LEN))\n",
    "    r_tr = align_y(y_train_ret.values.astype(np.float32), SEQ_LEN)\n",
    "    z_seed.append(p_tr - 0.5); r_seed.append(r_tr)\n",
    "\n",
    "    p_va = predict_proba(model, make_seq_2d_to_3d(Xva2, SEQ_LEN))\n",
    "    r_va = align_y(y_val_ret.values.astype(np.float32), SEQ_LEN)\n",
    "    z_seed.append(p_va - 0.5); r_seed.append(r_va)\n",
    "\n",
    "    z_seed = np.concatenate(z_seed) if len(z_seed)>0 else np.zeros(0)\n",
    "    r_seed = np.concatenate(r_seed) if len(r_seed)>0 else np.zeros(0)\n",
    "\n",
    "    cal = OnlineCalibrator(ridge=RIDGE_L2, cap_b=B_CAP, win=CAL_WIN)\n",
    "    if len(z_seed) > 0:\n",
    "        cal.fit_from_arrays(z_seed, r_seed)\n",
    "\n",
    "    # Rolling forecast on Test (model params frozen)\n",
    "    pred_prob = pd.Series(index=test_idx, dtype=float)\n",
    "    pred_rhat = pd.Series(index=test_idx, dtype=float)\n",
    "\n",
    "    ret_hist  = deque(maxlen=MEAN_MATCH_WIN)\n",
    "    rhat_hist = deque(maxlen=MEAN_MATCH_WIN)\n",
    "\n",
    "    z_prev, prev_j = None, None\n",
    "    for j in test_idx:\n",
    "        X_hist = X_all.loc[:j].values\n",
    "        if len(X_hist) < SEQ_LEN: \n",
    "            continue\n",
    "        X_hist_sc = scaler.transform(X_hist)\n",
    "        X_last3   = make_seq_2d_to_3d(X_hist_sc, SEQ_LEN)[-1:,:,:]\n",
    "\n",
    "        p = float(predict_proba(model, X_last3)[0])\n",
    "        z = p - 0.5\n",
    "\n",
    "        coef = cal.coef()\n",
    "        a_t, b_t = (0.0, 0.0) if coef is None else coef\n",
    "\n",
    "        rhat = a_t + b_t * z\n",
    "        rhat = online_mean_match(rhat, ret_hist, rhat_hist)\n",
    "        rhat = float(np.clip(rhat, -RHAT_CAP, RHAT_CAP))\n",
    "\n",
    "        pred_prob.loc[j] = p\n",
    "        pred_rhat.loc[j] = rhat\n",
    "\n",
    "        # Update calibrator and mean-match buffers with yesterday's realized data\n",
    "        if z_prev is not None and prev_j is not None:\n",
    "            cal.add(z_prev, float(get_by_key(ret_all, prev_j)))\n",
    "            ret_hist.append(float(get_by_key(ret_all, prev_j)))\n",
    "            rhat_hist.append(float(pred_rhat.loc[prev_j]))\n",
    "        z_prev, prev_j = z, j\n",
    "\n",
    "    # Build synthetic price on Test (shift(1) to apply on next day)\n",
    "    valid = pred_prob.dropna().index\n",
    "    if len(valid)==0:\n",
    "        print(f\"\\n[{TICKER}] no valid test predictions, skip.\")\n",
    "        continue\n",
    "\n",
    "    ret_oos   = y_test_ret.loc[valid]\n",
    "    close_oos = close_all.loc[valid]\n",
    "\n",
    "    P0 = float(close_oos.iloc[0])\n",
    "    pred_factor = (1.0 + pred_rhat.loc[valid]).cumprod().shift(1).fillna(1.0)\n",
    "    pred_price  = pd.Series(P0, index=pred_factor.index) * pred_factor\n",
    "\n",
    "    # Scheme A rebasing (display only)\n",
    "    dfp = pd.DataFrame({\"actual\": close_oos, \"pred\": pred_price}).dropna().copy()\n",
    "    blocks_curve = np.arange(len(dfp)) // REBASE_DAYS_FOR_CURVE\n",
    "    def _rebase_block(g: pd.DataFrame):\n",
    "        s = float(g[\"actual\"].iloc[0] / g[\"pred\"].iloc[0])\n",
    "        g[\"pred_rb\"] = g[\"pred\"] * s\n",
    "        return g\n",
    "    dfp = dfp.groupby(blocks_curve, group_keys=False).apply(_rebase_block)\n",
    "    pred_price_rb = dfp[\"pred_rb\"]\n",
    "\n",
    "    # Save test csv: Date, PredictedPrice\n",
    "    out = pd.DataFrame({\"Date\": valid, \"PredictedPrice\": pred_price.loc[valid].values})\n",
    "    out.to_csv(os.path.join(OUTPUT_DIR, f\"{TICKER}.csv\"), index=False)\n",
    "    print(f\"\\n[{TICKER}] saved -> {TICKER}.csv  (test period)\")\n",
    "\n",
    "    # For metrics\n",
    "    price_actual = close_oos\n",
    "    price_pred   = pred_price.loc[valid]\n",
    "\n",
    "    # OOS metrics on returns\n",
    "    price_err = price_actual - price_pred\n",
    "    mse_model = float((price_err ** 2).mean())\n",
    "    mse_base  = float(((price_actual - price_actual.mean()) ** 2).mean())\n",
    "    r2_oos    = 1 - mse_model / mse_base if mse_base > 0 else np.nan\n",
    "    ic        = float(pd.Series(price_pred).corr(price_actual))\n",
    "\n",
    "    price_diff = price_actual.diff()\n",
    "    pred_diff  = price_pred.diff()\n",
    "    common_idx = price_diff.dropna().index.intersection(pred_diff.dropna().index)\n",
    "    hit        = float((np.sign(pred_diff.loc[common_idx]) == np.sign(price_diff.loc[common_idx])).mean())\n",
    "\n",
    "    # mse_model = float(((ret_oos - pred_rhat.loc[valid])**2).mean())\n",
    "    # mse_base  = float(((ret_oos - ret_oos.mean())**2).mean())\n",
    "    # r2_oos    = 1 - mse_model/mse_base if mse_base>0 else np.nan\n",
    "    # ic        = float(pd.Series(pred_rhat.loc[valid]).corr(ret_oos))\n",
    "    # hit       = float((np.sign(pred_rhat.loc[valid])==np.sign(ret_oos)).mean())\n",
    "\n",
    "    # Directional Sharpe (reference only)\n",
    "    str_ret = np.sign(pred_rhat.loc[valid]) * ret_oos\n",
    "    ann_ret = (1+str_ret.mean())**252 - 1\n",
    "    ann_vol = str_ret.std(ddof=0) * np.sqrt(252)\n",
    "    sharpe  = float(ann_ret/ann_vol) if ann_vol>0 else np.nan\n",
    "\n",
    "    print(f\"[{TICKER}] TEST  MSE={mse_model:.3e} | R2={r2_oos: .3f} | IC={ic: .3f} | Hit={hit: .3f} | Sharpe={sharpe: .2f}\")\n",
    "\n",
    "    summary.append([TICKER, mse_model, r2_oos, ic, hit, sharpe, valid[0].date(), valid[-1].date(), len(valid)])\n",
    "\n",
    "    # Plot: Actual vs Predicted (rebased)\n",
    "    plt.figure(figsize=(10,3.2))\n",
    "    plt.plot(close_oos.index, close_oos.values, label=\"Actual Close (USD)\")\n",
    "    plt.plot(pred_price_rb.index, pred_price_rb.values,\n",
    "             label=f\"Predicted (rebased {REBASE_DAYS_FOR_CURVE}d)\")\n",
    "    plt.title(f\"{TICKER} | Actual vs Predicted (TEST)\")\n",
    "    plt.ylabel(\"USD\"); plt.legend(); plt.grid(True, alpha=0.3)\n",
    "    fn = os.path.join(OUTPUT_DIR, f\"{TICKER}_TEST_curve.png\")\n",
    "    plt.tight_layout(); plt.savefig(fn, dpi=140); plt.close()\n",
    "    print(f\"[{TICKER}] saved -> {os.path.basename(fn)}\")\n",
    "\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# ---------------- Summary ----------------\n",
    "sum_df = pd.DataFrame(summary, columns=[\n",
    "    \"Ticker\",\"MSE\",\"R2_TEST\",\"IC\",\"HitRate\",\"Sharpe\",\"TEST_start\",\"TEST_end\",\"#TEST_days\"\n",
    "])\n",
    "sum_df = sum_df.sort_values(\"R2_TEST\", ascending=False)\n",
    "sum_df.to_csv(os.path.join(OUTPUT_DIR, \"OOS_summary.csv\"), index=False)\n",
    "print(\"\\nSaved OOS_summary.csv\")\n",
    "print(sum_df.head(20).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28015f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tickers discovered: 12  → using first 5: ['AAPL', 'AMZN', 'AVGO', 'GOOGL', 'META']\n",
      "Saved: f:\\Master\\Book\\DSA5205\\Project 2\\code\\qf_project2\\GRU\\output\\diag_price_all\\TOP5_hit_60d.png\n",
      "Saved: f:\\Master\\Book\\DSA5205\\Project 2\\code\\qf_project2\\GRU\\output\\diag_price_all\\TOP5_ic_60d.png\n",
      "Saved: f:\\Master\\Book\\DSA5205\\Project 2\\code\\qf_project2\\GRU\\output\\diag_price_all\\TOP5_mse_60d.png\n",
      "Saved: f:\\Master\\Book\\DSA5205\\Project 2\\code\\qf_project2\\GRU\\output\\diag_price_all\\TOP5_relative_price_error.png\n"
     ]
    }
   ],
   "source": [
    "# ===== plots_all_curves_price_metrics.py =====\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------------------- Config (edit paths if needed) ----------------------\n",
    "PRICE_FILE  = os.path.join(DATA_DIR, \"price_data_full.csv\")\n",
    "OUT_DIR     = os.path.join(OUTPUT_DIR, \"diag_price_all\")\n",
    "WIN         = 60  # rolling window for metrics\n",
    "MAX_TICKERS = 5\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# ---------------------- Data loading helpers ----------------------\n",
    "def read_price_panel(price_file: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read panel CSV with a two-level header [PriceField, Ticker].\n",
    "    Returns a DataFrame indexed by datetime, columns are MultiIndex.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(price_file, header=[0, 1], index_col=0)\n",
    "    if isinstance(df.index[0], str) and str(df.index[0]).strip().lower() == \"date\":\n",
    "        df = df.iloc[1:]\n",
    "    df.index = pd.to_datetime(df.index, errors=\"coerce\")\n",
    "    df = df[~df.index.isna()].sort_index()\n",
    "    return df\n",
    "\n",
    "def load_actual_close(df_panel: pd.DataFrame, ticker: str) -> pd.Series:\n",
    "    \"\"\"Get Close series for a ticker (float Series indexed by date).\"\"\"\n",
    "    return pd.to_numeric(df_panel[(\"Close\", ticker)], errors=\"coerce\")\n",
    "\n",
    "def load_predicted_price(output_dir: str, ticker: str) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Read {OUTPUT_DIR}/{TICKER}.csv with columns [Date, PredictedPrice].\n",
    "    Return a float Series indexed by Date.\n",
    "    \"\"\"\n",
    "    fp = os.path.join(output_dir, f\"{ticker}.csv\")\n",
    "    if not os.path.exists(fp):\n",
    "        raise FileNotFoundError(fp)\n",
    "    g = pd.read_csv(fp)\n",
    "    if \"Date\" not in g.columns or \"PredictedPrice\" not in g.columns:\n",
    "        raise ValueError(f\"{fp} must contain columns: Date, PredictedPrice\")\n",
    "    g[\"Date\"] = pd.to_datetime(g[\"Date\"])\n",
    "    g = g.dropna(subset=[\"Date\"]).sort_values(\"Date\")\n",
    "    return pd.Series(g[\"PredictedPrice\"].astype(float).values, index=g[\"Date\"], name=\"PredictedPrice\")\n",
    "\n",
    "# ---------------------- Plotting helpers ----------------------\n",
    "def make_color_cycle(n: int):\n",
    "    \"\"\"\n",
    "    Build an aesthetic color list long enough for n tickers:\n",
    "    use tab20, tab20b, tab20c first, then fallback to HSV.\n",
    "    \"\"\"\n",
    "    palettes = []\n",
    "    for name in [\"tab20\", \"tab20b\", \"tab20c\"]:\n",
    "        cmap = plt.get_cmap(name)\n",
    "        palettes.extend(list(cmap.colors))\n",
    "    if n <= len(palettes):\n",
    "        return palettes[:n]\n",
    "    # fallback: evenly spaced HSV\n",
    "    return [plt.cm.hsv(i / max(1, n)) for i in range(n)]\n",
    "\n",
    "def plot_multicurve(series_map, title, ylabel, fname, use_logy=False, legend_cols=4):\n",
    "    \"\"\"\n",
    "    Plot multiple time series (dict: {ticker: pd.Series}) on one figure.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    colors = make_color_cycle(len(series_map))\n",
    "    for (ticker, s), c in zip(series_map.items(), colors):\n",
    "        s = s.dropna()\n",
    "        if len(s) == 0:\n",
    "            continue\n",
    "        plt.plot(s.index, s.values, label=ticker, lw=1.6, alpha=0.95, color=c)\n",
    "    if use_logy:\n",
    "        plt.yscale(\"log\")\n",
    "    plt.title(title)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    # place legend at bottom to reduce occlusion\n",
    "    plt.legend(loc=\"upper center\", bbox_to_anchor=(0.5, -0.12), ncol=legend_cols, fontsize=8)\n",
    "    plt.tight_layout()\n",
    "    fp = os.path.join(OUT_DIR, fname)\n",
    "    plt.savefig(fp, dpi=160, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(f\"Saved: {fp}\")\n",
    "\n",
    "# ---------------------- Core computation ----------------------\n",
    "def compute_rolling_metrics_all(df_panel: pd.DataFrame, tickers: list, win: int = 60):\n",
    "    \"\"\"\n",
    "    For each ticker, compute:\n",
    "      - hit_rolling (price-diff direction hit-rate, rolling mean)\n",
    "      - ic_rolling  (price-level correlation, rolling)\n",
    "      - mse_rolling (price-level squared error rolling mean)\n",
    "      - rel_err     (Actual/Pred - 1), not rolling (raw curve)\n",
    "    Returns four dicts mapping ticker -> pd.Series.\n",
    "    \"\"\"\n",
    "    hit_curves = {}\n",
    "    ic_curves  = {}\n",
    "    mse_curves = {}\n",
    "    rel_curves = {}\n",
    "\n",
    "    for tkr in tickers:\n",
    "        # Load price series\n",
    "        try:\n",
    "            pa = load_actual_close(df_panel, tkr).dropna()\n",
    "        except KeyError:\n",
    "            print(f\"[{tkr}] missing Close in PRICE_FILE, skipped.\")\n",
    "            continue\n",
    "        try:\n",
    "            pp = load_predicted_price(OUTPUT_DIR, tkr).dropna()\n",
    "        except (FileNotFoundError, ValueError):\n",
    "            print(f\"[{tkr}] missing predicted CSV, skipped.\")\n",
    "            continue\n",
    "\n",
    "        # Align by common dates\n",
    "        idx = pa.index.intersection(pp.index)\n",
    "        if len(idx) < max(40, win + 5):\n",
    "            print(f\"[{tkr}] too few overlapping dates, skipped.\")\n",
    "            continue\n",
    "        pa = pa.loc[idx].astype(float)\n",
    "        pp = pp.loc[idx].astype(float)\n",
    "\n",
    "        # Rolling MSE on price\n",
    "        price_res = (pa - pp)\n",
    "        mse_s = (price_res ** 2).rolling(win).mean()\n",
    "\n",
    "        # Rolling IC on price (correlation)\n",
    "        ic_s = pp.rolling(win).corr(pa)\n",
    "\n",
    "        # Rolling Hit on price-diff direction\n",
    "        pa_diff = pa.diff()\n",
    "        pp_diff = pp.diff()\n",
    "        diff_idx = pa_diff.dropna().index.intersection(pp_diff.dropna().index)\n",
    "        hit_series = (np.sign(pp_diff.loc[diff_idx]) == np.sign(pa_diff.loc[diff_idx])).astype(float)\n",
    "        hit_s = hit_series.rolling(win).mean()\n",
    "\n",
    "        # Relative price error (Actual/Pred - 1), raw series\n",
    "        rel_err = (pa / pp) - 1.0\n",
    "\n",
    "        hit_curves[tkr] = hit_s\n",
    "        ic_curves[tkr]  = ic_s\n",
    "        mse_curves[tkr] = mse_s\n",
    "        rel_curves[tkr] = rel_err\n",
    "\n",
    "    return hit_curves, ic_curves, mse_curves, rel_curves\n",
    "\n",
    "# ---------------------- Main ----------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Load panel and discover tickers with prediction CSVs\n",
    "    panel = read_price_panel(PRICE_FILE)\n",
    "\n",
    "    # tickers listed by predicted CSVs (ensures availability)\n",
    "    out_csvs = [f for f in os.listdir(OUTPUT_DIR) if f.endswith(\".csv\") and f != \"OOS_summary.csv\"]\n",
    "    tickers = sorted({os.path.splitext(f)[0] for f in out_csvs})\n",
    "\n",
    "    if len(tickers) == 0:\n",
    "        raise RuntimeError(\"No {TICKER}.csv found in OUTPUT_DIR.\")\n",
    "\n",
    "    # Only use first MAX_TICKERS for plotting\n",
    "    selected = tickers[:MAX_TICKERS]\n",
    "    print(f\"Tickers discovered: {len(tickers)}  → using first {len(selected)}: {selected}\")\n",
    "\n",
    "    # Only compute metrics for selected tickers\n",
    "    hit_map, ic_map, mse_map, rel_map = compute_rolling_metrics_all(panel, selected, win=WIN)\n",
    "\n",
    "    # Plot (file names can have a prefix for distinction)\n",
    "    plot_multicurve(hit_map, f\"Rolling Hit-rate on Price Diff ({WIN}d)\", f\"Hit({WIN}d)\", \"TOP5_hit_60d.png\")\n",
    "    plot_multicurve(ic_map,  f\"Rolling IC on Price Level ({WIN}d)\",    f\"IC({WIN}d)\",  \"TOP5_ic_60d.png\")\n",
    "    plot_multicurve(mse_map, f\"Rolling MSE on Price ({WIN}d)\",         f\"MSE({WIN}d)\", \"TOP5_mse_60d.png\", use_logy=False)\n",
    "    plot_multicurve(rel_map, \"Relative Price Error (Actual/Pred − 1)\", \"Rel. error\",   \"TOP5_relative_price_error.png\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSA5205_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
